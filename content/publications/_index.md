---
date: '2025-01-16T12:00:00+02:00'
ShowDate: false
draft: false
ShowBreadCrumbs: false
ShowWordCount: false
ShowReadingTime: false
ShowShareButtons: false
hidemeta: true
---

# Publications

Papers listed by type and year of publication or year of submission if unpublished.

---

## Preprints

### [Tighter performance theory of FedExProx](https://arxiv.org/abs/2410.15368)

**Authors:**  Wojciech Anyszka, Kaja Gruntkowska, Alexander Tyurin, Peter Richtárik
**Source:** arXiv:2410.15368, 2024.

</details>
<table class="publication-actions">
<tr>
<td><button class="action-btn" onclick="toggleContent('abstract1')">Abstract</button></td>
<td><button class="action-btn" onclick="toggleContent('bibtex1')">BibTeX</button></td>
<td><button class="action-btn" onclick="downloadFile('https://arxiv.org/pdf/2410.15368')">Download</a></td>
</tr>
</table>

<div id="abstract1" class="expandable-content" style="display: none;">
<strong>Abstract:</strong><br>
    We revisit FedExProx — a recently proposed distributed optimization method designed to enhance convergence properties of parallel proximal algorithms via extrapolation. In the process, we uncover a surprising flaw: its known theoretical guarantees on quadratic optimization tasks are no better than those offered by the vanilla Gradient Descent (GD) method. Motivated by this observation, we develop a novel analysis framework, establishing a tighter linear convergence rate for non-strongly convex quadratic problems. By incorporating both computation and communication costs, we demonstrate that FedExProx can indeed provably outperform GD, in stark contrast to the original analysis. Furthermore, we consider partial participation scenarios and analyze two adaptive extrapolation strategies — based on gradient diversity and Polyak stepsizes — again significantly outperforming previous results. Moving beyond quadratics, we extend the applicability of our analysis to general functions satisfying the Polyak-Łojasiewicz condition, outperforming the previous strongly convex analysis while operating under weaker assumptions. Backed by empirical results, our findings point to a new and stronger potential of FedExProx, paving the way for further exploration of the benefits of extrapolation in federated learning.
</div>

<div id="bibtex1" class="expandable-content" style="display: none;">
<strong>BibTeX:</strong><br>
<pre><code>@misc{anyszka2024tighterperformancetheoryfedexprox,
      title={Tighter Performance Theory of FedExProx}, 
      author={Wojciech Anyszka and Kaja Gruntkowska and Alexander Tyurin and Peter Richtárik},
      year={2024},
      eprint={2410.15368},
      archivePrefix={arXiv},
      primaryClass={math.OC},
      url={https://arxiv.org/abs/2410.15368}, 
}</code></pre>
</div>


---


## Theses

*Coming soon...*